{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "import transformers\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BertTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    DebertaTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    DebertaForSequenceClassification,\n",
    "    DebertaForMaskedLM\n",
    ")\n",
    "from tokenizers import (\n",
    "    BertWordPieceTokenizer\n",
    ")\n",
    "from hybrid_embedding_modeling import (\n",
    "    preprocess_for_hybrid_embeddings,\n",
    "    preprocess_for_inclusive_embeddings,\n",
    "    HybridEmbeddingModel\n",
    ")\n",
    "from activated_attention_modeling import (\n",
    "    ActivatedAttentionForSequenceClassification, \n",
    "    AAConfig,\n",
    "    ActivatedAttentionForMaskedLM\n",
    ")\n",
    "from utils import (\n",
    "    get_act_func, \n",
    "    Dataloader, \n",
    "    train_run, \n",
    "    preprocess_with_given_labels, \n",
    "    preprocess_with_given_labels_train_test_wrap,\n",
    "    num_parameters,\n",
    "    num_trainable_parameters,\n",
    "    preprocess_for_key_masking,\n",
    "    preprocess_for_maskedlm,\n",
    "    #prn_fn\n",
    ")\n",
    "from load_set import load_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "CHECKPOINT_PATH = None\n",
    "SHUFFLE_CUSTOM_DATALOADER = True\n",
    "# use slightly higher lr on mlm\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 20\n",
    "EMPTY_CACHE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOCAB_SIZE = 30_522\n",
    "#VOCAB_SIZE = 50257\n",
    "VOCAB_SIZE = 50265 \n",
    "MAX_POSITION_EMBEDDINGS = 514\n",
    "HIDDEN_SIZE = 1024\n",
    "GENERIC_OUTPUT_CLASS = True\n",
    "DOC_PAD_TOKENS = False\n",
    "\n",
    "NUM_LABELS = 30\n",
    "CALC_METRICS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_args = [\"input_ids\", \"attention_mask\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"trained_models/deberta_inbio_def_mlm_lr1e-5/\"\n",
    "WORDPIECE_TOKENIZER_DIR = f\"{CHECKPOINT_PATH}/tokenizer_wordpiece/\"\n",
    "try:\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "    os.mkdir(WORDPIECE_TOKENIZER_DIR)\n",
    "except:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    #model_path = \"../cc-phoebe/tmp_models/COIN-i3C_default_tokenizer/\"\n",
    "    #tokenizer = BertTokenizer.from_pretrained(f\"{model_path}/wordpiece_tokenizer/\")\n",
    "    #VOCAB_SIZE = tokenizer.vocab_size\n",
    "    model = ActivatedAttentionForMaskedLM(config=AAConfig(\n",
    "        num_layers=2,\n",
    "        hidden_act=\"tanh\",\n",
    "        group_norm_eps=1e-8,\n",
    "        num_labels=NUM_LABELS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_norm_groups=8,\n",
    "        num_norm_channels=HIDDEN_SIZE,\n",
    "    ))\n",
    "    CALC_METRICS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    #tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    print(tokenizer.vocab_size)\n",
    "    model_path = \"../cc-phoebe/tmp_models/COIN-i3C_default_tokenizer/\"\n",
    "    #tokenizer = BertTokenizer.from_pretrained(f\"{model_path}/wordpiece_tokenizer/\")\n",
    "    #VOCAB_SIZE = tokenizer.vocab_size\n",
    "    model = ActivatedAttentionForSequenceClassification(config=AAConfig(\n",
    "        num_layers=2,\n",
    "        hidden_act=\"tanh\",\n",
    "        group_norm_eps=1e-8,\n",
    "        num_labels=NUM_LABELS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_norm_groups=8,\n",
    "        num_norm_channels=HIDDEN_SIZE,\n",
    "    ))\n",
    "    CHECKPOINT_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    #tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    #tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    #model_path = \"trained_models/aa_mlm_lr1e-4_debertatok/\"\n",
    "    model_path = CHECKPOINT_PATH\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{model_path}/tokenizer_wordpiece/\")\n",
    "    #VOCAB_SIZE = tokenizer.vocab_size\n",
    "    model = ActivatedAttentionForSequenceClassification.from_pretrained(\n",
    "        f\"{model_path}/iter_04/\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )\n",
    "    CHECKPOINT_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    model = DebertaForMaskedLM.from_pretrained(\n",
    "        \"microsoft/deberta-base\"\n",
    "    )\n",
    "    CALC_METRICS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    model = DebertaForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/deberta-base\",\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\" # uses BCEWIthLogitsLoss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at trained_models/deberta_inbio_def_mlm_lr1e-5//iter_07/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    model = DebertaForSequenceClassification.from_pretrained(\n",
    "        f\"{CHECKPOINT_PATH}/iter_07/\",\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "    CHECKPOINT_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139,215,390\n",
      "139,215,390\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\\n{:,}\".format(num_parameters(model), num_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "    INBIO.csv\n"
     ]
    }
   ],
   "source": [
    "inbio = load_set([\"INBIO.csv\"], unused_fields=(\"Synonyms,Obsolete,CUI,Semantic Types,Parents,achieves,adjacent to,affects,allocates,capable of,characteristic for,completed invasion phase,contained in,contains,contributes to,contributor,created by,decreases,decreases effort in,derives from,derives into,determines,don't use concept,editor note,enabled by,ends,ends during,ends with,enhance,facilitate,has alien range,has amount of closely related species,has amount of species,has area,has component,has decreased effort level by,has distribution,has growth,has habitat,has increased effort level by,has increased levels of,has index,has input,has invasion success likelihood,has level of,has measurement,has measurement unit label,has measurement value,has mortality,has natality,has native range,has number of individuals,has output,has part,has part structure that is capable of,has participant,has propagule pressure,has quality,has range,has recruitment,has role,has spatial occupant at some time,has specific name,has status,has value,http://data.bioontology.org/metadata/obo/part_of,http://data.bioontology.org/metadata/prefixIRI,http://data.bioontology.org/metadata/treeView,http://purl.obolibrary.org/obo/IAO_0000111,http://purl.obolibrary.org/obo/IAO_0000112,http://purl.obolibrary.org/obo/IAO_0000114,http://purl.obolibrary.org/obo/IAO_0000115,http://purl.obolibrary.org/obo/IAO_0000118,http://purl.obolibrary.org/obo/IAO_0000119,http://purl.obolibrary.org/obo/IAO_0000232,http://purl.obolibrary.org/obo/IAO_0000412,http://purl.obolibrary.org/obo/ncbitaxon#has_rank,http://purl.obolibrary.org/obo/NCIT_A8,http://purl.obolibrary.org/obo/NCIT_NHC0,http://purl.obolibrary.org/obo/NCIT_P106,http://purl.obolibrary.org/obo/NCIT_P107,http://purl.obolibrary.org/obo/NCIT_P108,http://purl.obolibrary.org/obo/NCIT_P207,http://purl.obolibrary.org/obo/NCIT_P322,http://purl.obolibrary.org/obo/NCIT_P325,http://purl.obolibrary.org/obo/NCIT_P366,http://purl.obolibrary.org/obo/OBI_0001886,http://purl.obolibrary.org/obo/RO_0001900,http://purl.org/dc/elements/1.1/source,http://purl.org/dc/terms/creator,http://www.geneontology.org/formats/oboInOwl#creation_date,http://www.geneontology.org/formats/oboInOwl#hasAlternativeId,http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym,http://www.geneontology.org/formats/oboInOwl#hasDbXref,http://www.geneontology.org/formats/oboInOwl#hasExactSynonym,http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym,http://www.geneontology.org/formats/oboInOwl#hasOBONamespace,http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym,http://www.geneontology.org/formats/oboInOwl#hasSynonymType,http://www.geneontology.org/formats/oboInOwl#id,http://www.geneontology.org/formats/oboInOwl#inSubset,http://www.w3.org/2000/01/rdf-schema#comment,http://www.w3.org/2000/01/rdf-schema#label,http://www.w3.org/2002/07/owl#deprecated,http://www.w3.org/2004/02/skos/core#altLabel,http://www.w3.org/2004/02/skos/core#definition,http://www.w3.org/2004/02/skos/core#notation,https://w3id.org/inbio#_000130,https://w3id.org/inbio#_000132,increases,increases effort in,interacts with,is absent,is affected by,is against,is aggregate of,is alien range to,is characteristic of,is characterized by,is closely related to,is enemy of,is enhanced by,is growth of,is habitat of,is in invasion phase,is mortality of,is natality of,is native range to,is part of,is prey of,is range of,is recruitment of,is similar to,is status of,license,license,license,license,located in,location of,occupies spatial region at some time,occurs in,output of,overlaps,part of,participates in,produced by,produces,quality of,role of,shows changes in species trait,spatially coextensive with,surrounded by,surrounds,title,TODO,license.1,license.2,license.3\".split(\",\")))\n",
    "bio2def = dict(zip(inbio[\"Preferred Label\"], inbio[\"Definitions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all labels: datasets/wordpiece_abstracts_train_all_labels.csv  30 labels\n",
    "# 2 labels: datasets/wordpiece_abstracts_train_side_label_1.csv  20 labels\n",
    "# main label only: datasets/wordpiece_abstracts_train.csv        10 labels\n",
    "\n",
    "#DS_TRAIN_PATH = \"datasets/wordpiece_abstracts_train_all_labels.csv\"\n",
    "#DS_TEST_PATH = \"datasets/wordpiece_abstracts_test_all_labels.csv\"\n",
    "\n",
    "DS_TRAIN_PATH = \"datasets/abstracts_all_labels_train.csv\"\n",
    "DS_TEST_PATH = \"datasets/abstracts_all_labels_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "    datasets/abstracts_all_labels_train.csv\n",
      "loading files\n",
      "    datasets/abstracts_all_labels_test.csv\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": load_set([DS_TRAIN_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "        \"test\": load_set([DS_TEST_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "    INBIO.csv\n"
     ]
    }
   ],
   "source": [
    "inbio = load_set([\"INBIO.csv\"], unused_fields=(\"Synonyms,Obsolete,CUI,Semantic Types,Parents,achieves,adjacent to,affects,allocates,capable of,characteristic for,completed invasion phase,contained in,contains,contributes to,contributor,created by,decreases,decreases effort in,derives from,derives into,determines,don't use concept,editor note,enabled by,ends,ends during,ends with,enhance,facilitate,has alien range,has amount of closely related species,has amount of species,has area,has component,has decreased effort level by,has distribution,has growth,has habitat,has increased effort level by,has increased levels of,has index,has input,has invasion success likelihood,has level of,has measurement,has measurement unit label,has measurement value,has mortality,has natality,has native range,has number of individuals,has output,has part,has part structure that is capable of,has participant,has propagule pressure,has quality,has range,has recruitment,has role,has spatial occupant at some time,has specific name,has status,has value,http://data.bioontology.org/metadata/obo/part_of,http://data.bioontology.org/metadata/prefixIRI,http://data.bioontology.org/metadata/treeView,http://purl.obolibrary.org/obo/IAO_0000111,http://purl.obolibrary.org/obo/IAO_0000112,http://purl.obolibrary.org/obo/IAO_0000114,http://purl.obolibrary.org/obo/IAO_0000115,http://purl.obolibrary.org/obo/IAO_0000118,http://purl.obolibrary.org/obo/IAO_0000119,http://purl.obolibrary.org/obo/IAO_0000232,http://purl.obolibrary.org/obo/IAO_0000412,http://purl.obolibrary.org/obo/ncbitaxon#has_rank,http://purl.obolibrary.org/obo/NCIT_A8,http://purl.obolibrary.org/obo/NCIT_NHC0,http://purl.obolibrary.org/obo/NCIT_P106,http://purl.obolibrary.org/obo/NCIT_P107,http://purl.obolibrary.org/obo/NCIT_P108,http://purl.obolibrary.org/obo/NCIT_P207,http://purl.obolibrary.org/obo/NCIT_P322,http://purl.obolibrary.org/obo/NCIT_P325,http://purl.obolibrary.org/obo/NCIT_P366,http://purl.obolibrary.org/obo/OBI_0001886,http://purl.obolibrary.org/obo/RO_0001900,http://purl.org/dc/elements/1.1/source,http://purl.org/dc/terms/creator,http://www.geneontology.org/formats/oboInOwl#creation_date,http://www.geneontology.org/formats/oboInOwl#hasAlternativeId,http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym,http://www.geneontology.org/formats/oboInOwl#hasDbXref,http://www.geneontology.org/formats/oboInOwl#hasExactSynonym,http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym,http://www.geneontology.org/formats/oboInOwl#hasOBONamespace,http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym,http://www.geneontology.org/formats/oboInOwl#hasSynonymType,http://www.geneontology.org/formats/oboInOwl#id,http://www.geneontology.org/formats/oboInOwl#inSubset,http://www.w3.org/2000/01/rdf-schema#comment,http://www.w3.org/2000/01/rdf-schema#label,http://www.w3.org/2002/07/owl#deprecated,http://www.w3.org/2004/02/skos/core#altLabel,http://www.w3.org/2004/02/skos/core#definition,http://www.w3.org/2004/02/skos/core#notation,https://w3id.org/inbio#_000130,https://w3id.org/inbio#_000132,increases,increases effort in,interacts with,is absent,is affected by,is against,is aggregate of,is alien range to,is characteristic of,is characterized by,is closely related to,is enemy of,is enhanced by,is growth of,is habitat of,is in invasion phase,is mortality of,is natality of,is native range to,is part of,is prey of,is range of,is recruitment of,is similar to,is status of,license,license,license,license,located in,location of,occupies spatial region at some time,occurs in,output of,overlaps,part of,participates in,produced by,produces,quality of,role of,shows changes in species trait,spatially coextensive with,surrounded by,surrounds,title,TODO,license.1,license.2,license.3\".split(\",\")))\n",
    "bio2def = dict(zip(inbio[\"Preferred Label\"], inbio[\"Definitions\"]))\n",
    "inbio_def_list = [{\"text\" : n} for n in inbio[\"Definitions\"] if n is not None]\n",
    "mask_keys = inbio[\"Preferred Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    inbio_dataset = Dataset.from_list(inbio_def_list)\n",
    "    dataset[\"train\"] = concatenate_datasets([dataset[\"train\"], inbio_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    inbio = load_set([\"INBIO.csv\"], unused_fields=(\"Synonyms,Obsolete,CUI,Semantic Types,Parents,achieves,adjacent to,affects,allocates,capable of,characteristic for,completed invasion phase,contained in,contains,contributes to,contributor,created by,decreases,decreases effort in,derives from,derives into,determines,don't use concept,editor note,enabled by,ends,ends during,ends with,enhance,facilitate,has alien range,has amount of closely related species,has amount of species,has area,has component,has decreased effort level by,has distribution,has growth,has habitat,has increased effort level by,has increased levels of,has index,has input,has invasion success likelihood,has level of,has measurement,has measurement unit label,has measurement value,has mortality,has natality,has native range,has number of individuals,has output,has part,has part structure that is capable of,has participant,has propagule pressure,has quality,has range,has recruitment,has role,has spatial occupant at some time,has specific name,has status,has value,http://data.bioontology.org/metadata/obo/part_of,http://data.bioontology.org/metadata/prefixIRI,http://data.bioontology.org/metadata/treeView,http://purl.obolibrary.org/obo/IAO_0000111,http://purl.obolibrary.org/obo/IAO_0000112,http://purl.obolibrary.org/obo/IAO_0000114,http://purl.obolibrary.org/obo/IAO_0000115,http://purl.obolibrary.org/obo/IAO_0000118,http://purl.obolibrary.org/obo/IAO_0000119,http://purl.obolibrary.org/obo/IAO_0000232,http://purl.obolibrary.org/obo/IAO_0000412,http://purl.obolibrary.org/obo/ncbitaxon#has_rank,http://purl.obolibrary.org/obo/NCIT_A8,http://purl.obolibrary.org/obo/NCIT_NHC0,http://purl.obolibrary.org/obo/NCIT_P106,http://purl.obolibrary.org/obo/NCIT_P107,http://purl.obolibrary.org/obo/NCIT_P108,http://purl.obolibrary.org/obo/NCIT_P207,http://purl.obolibrary.org/obo/NCIT_P322,http://purl.obolibrary.org/obo/NCIT_P325,http://purl.obolibrary.org/obo/NCIT_P366,http://purl.obolibrary.org/obo/OBI_0001886,http://purl.obolibrary.org/obo/RO_0001900,http://purl.org/dc/elements/1.1/source,http://purl.org/dc/terms/creator,http://www.geneontology.org/formats/oboInOwl#creation_date,http://www.geneontology.org/formats/oboInOwl#hasAlternativeId,http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym,http://www.geneontology.org/formats/oboInOwl#hasDbXref,http://www.geneontology.org/formats/oboInOwl#hasExactSynonym,http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym,http://www.geneontology.org/formats/oboInOwl#hasOBONamespace,http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym,http://www.geneontology.org/formats/oboInOwl#hasSynonymType,http://www.geneontology.org/formats/oboInOwl#id,http://www.geneontology.org/formats/oboInOwl#inSubset,http://www.w3.org/2000/01/rdf-schema#comment,http://www.w3.org/2000/01/rdf-schema#label,http://www.w3.org/2002/07/owl#deprecated,http://www.w3.org/2004/02/skos/core#altLabel,http://www.w3.org/2004/02/skos/core#definition,http://www.w3.org/2004/02/skos/core#notation,https://w3id.org/inbio#_000130,https://w3id.org/inbio#_000132,increases,increases effort in,interacts with,is absent,is affected by,is against,is aggregate of,is alien range to,is characteristic of,is characterized by,is closely related to,is enemy of,is enhanced by,is growth of,is habitat of,is in invasion phase,is mortality of,is natality of,is native range to,is part of,is prey of,is range of,is recruitment of,is similar to,is status of,license,license,license,license,located in,location of,occupies spatial region at some time,occurs in,output of,overlaps,part of,participates in,produced by,produces,quality of,role of,shows changes in species trait,spatially coextensive with,surrounded by,surrounds,title,TODO,license.1,license.2,license.3\".split(\",\")))\n",
    "    bio2def = dict(zip(inbio[\"Preferred Label\"], inbio[\"Definitions\"]))\n",
    "    mask_keys = inbio[\"Preferred Label\"]\n",
    "\n",
    "    DS_TRAIN_PATH = \"datasets/abstracts_all_labels_train.csv\"\n",
    "    DS_TEST_PATH = \"datasets/abstracts_all_labels_test.csv\"\n",
    "\n",
    "    train_dataset = load_set([DS_TRAIN_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"])\n",
    "    #test_dataset = load_set([DS_TEST_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"])\n",
    "\n",
    "    #tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "    tok_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a0': 0, 'a1': 1, 'a2': 2, 'a3': 3, 'a4': 4, 'a5': 5, 'a6': 6, 'a7': 7, 'a8': 8, 'a9': 9, 'b0': 10, 'b1': 11, 'b2': 12, 'b3': 13, 'b4': 14, 'b5': 15, 'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'c0': 20, 'c1': 21, 'c2': 22, 'c3': 23, 'c4': 24, 'c5': 25, 'c6': 26, 'c7': 27, 'c8': 28, 'c9': 29}\n",
      "{0: 'a0', 1: 'a1', 2: 'a2', 3: 'a3', 4: 'a4', 5: 'a5', 6: 'a6', 7: 'a7', 8: 'a8', 9: 'a9', 10: 'b0', 11: 'b1', 12: 'b2', 13: 'b3', 14: 'b4', 15: 'b5', 16: 'b6', 17: 'b7', 18: 'b8', 19: 'b9', 20: 'c0', 21: 'c1', 22: 'c2', 23: 'c3', 24: 'c4', 25: 'c5', 26: 'c6', 27: 'c7', 28: 'c8', 29: 'c9'}\n",
      "['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True,\n",
    "                                        strip_accents=True, lowercase=True)\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[\n",
    "        \"[PAD]\", \n",
    "        \"[UNK]\", \n",
    "        \"[CLS]\", \n",
    "        \"[SEP]\", \n",
    "        \"[MASK]\"\n",
    "    ])\n",
    "    tokenizer.save_model(WORDPIECE_TOKENIZER_DIR)\n",
    "    tokenizer = BertTokenizer.from_pretrained(WORDPIECE_TOKENIZER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoded_dataset = preprocess_for_hybrid_embeddings(\n",
    "        dataset, tokenizer, labels, MAX_POSITION_EMBEDDINGS, incontext_dict=bio2def, remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoded_dataset = preprocess_with_given_labels_train_test_wrap(\n",
    "        dataset, tokenizer, labels, label2id, MAX_POSITION_EMBEDDINGS, one_label_only=False, remove_columns=dataset[\"train\"].column_names, \n",
    "        default_teacher_forcing=False, teacher_forcing_prefix=None, doc_pad_tokens=DOC_PAD_TOKENS,\n",
    "        incontext_dict=bio2def, move_incontext_to_decoder=False\n",
    "    )\n",
    "    encoded_dataset                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prn_fn(dataset, tokenizer, labels, max_length, remove_columns, text_field=\"text\"):\n",
    "    def proc(examples):\n",
    "        text = examples[text_field]\n",
    "        encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        \n",
    "        labels_batch = {k: v for k, v in examples.items() if k in labels}\n",
    "        labels_matrix = np.zeros((len(text), len(labels)))\n",
    "        for idx, label in enumerate(labels):\n",
    "            labels_matrix[:, idx] = labels_batch[label]\n",
    "        encoding[\"labels\"] = labels_matrix.tolist()\n",
    "        return encoding\n",
    "\n",
    "    return dataset.map(proc, batched=True, num_proc=4, remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESS_FN = prn_fn\n",
    "#PREPROCESS_FN = preprocess_for_masked_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e992ec2b7024fa5b5ee1a163cadfadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef3014575ca487f87d12aa899ca84f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 1:\n",
    "    encoded_dataset = DatasetDict({\n",
    "        \"train\": prn_fn(dataset[\"train\"], tokenizer, labels, MAX_POSITION_EMBEDDINGS, dataset[\"train\"].column_names),\n",
    "        \"test\": prn_fn(dataset[\"test\"], tokenizer, labels, MAX_POSITION_EMBEDDINGS, dataset[\"test\"].column_names)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoded_dataset = DatasetDict({\n",
    "        \"train\": preprocess_for_maskedlm(dataset[\"train\"], tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=dataset[\"train\"].column_names),\n",
    "        \"test\": preprocess_for_maskedlm(dataset[\"test\"], tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=dataset[\"test\"].column_names)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoded_dataset = DatasetDict({\n",
    "        \"train\": preprocess_for_key_masking(mask_keys, dataset[\"train\"], tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=dataset[\"train\"].column_names),\n",
    "        \"test\": preprocess_for_key_masking(mask_keys, dataset[\"test\"], tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=dataset[\"test\"].column_names)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoded_dataset = prn_fn(dataset[\"train\"], tokenizer, labels, MAX_POSITION_EMBEDDINGS, \n",
    "        dataset[\"train\"].column_names\n",
    "    ).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoded_dataset = preprocess_for_maskedlm(dataset[\"train\"], tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=dataset[\"train\"].column_names).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "print(batch_schema)\n",
    "train_dataloader = Dataloader(encoded_dataset[\"train\"], TRAIN_BATCH_SIZE)\n",
    "if SHUFFLE_CUSTOM_DATALOADER:\n",
    "    train_dataloader.shuffle()\n",
    "test_dataloader = Dataloader(encoded_dataset[\"test\"], TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataloader) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "  Batch    14  of    722.    Elapsed:  0:00:02, Remaining:  0:01:41.\n",
      "  Batch    28  of    722.    Elapsed:  0:00:03, Remaining:  0:00:50.\n",
      "  Batch    42  of    722.    Elapsed:  0:00:04, Remaining:  0:00:49.\n",
      "  Batch    56  of    722.    Elapsed:  0:00:06, Remaining:  0:00:48.\n",
      "  Batch    70  of    722.    Elapsed:  0:00:07, Remaining:  0:00:47.\n",
      "  Batch    84  of    722.    Elapsed:  0:00:08, Remaining:  0:00:46.\n",
      "  Batch    98  of    722.    Elapsed:  0:00:10, Remaining:  0:00:45.\n",
      "  Batch   112  of    722.    Elapsed:  0:00:11, Remaining:  0:00:44.\n",
      "  Batch   126  of    722.    Elapsed:  0:00:12, Remaining:  0:00:43.\n",
      "  Batch   140  of    722.    Elapsed:  0:00:14, Remaining:  0:00:42.\n",
      "  Batch   154  of    722.    Elapsed:  0:00:15, Remaining:  0:00:41.\n",
      "  Batch   168  of    722.    Elapsed:  0:00:17, Remaining:  0:00:40.\n",
      "  Batch   182  of    722.    Elapsed:  0:00:18, Remaining:  0:00:39.\n",
      "  Batch   196  of    722.    Elapsed:  0:00:19, Remaining:  0:00:38.\n",
      "  Batch   210  of    722.    Elapsed:  0:00:21, Remaining:  0:00:37.\n",
      "  Batch   224  of    722.    Elapsed:  0:00:22, Remaining:  0:00:36.\n",
      "  Batch   238  of    722.    Elapsed:  0:00:23, Remaining:  0:00:35.\n",
      "  Batch   252  of    722.    Elapsed:  0:00:25, Remaining:  0:00:34.\n",
      "  Batch   266  of    722.    Elapsed:  0:00:26, Remaining:  0:00:33.\n",
      "  Batch   280  of    722.    Elapsed:  0:00:28, Remaining:  0:00:32.\n",
      "  Batch   294  of    722.    Elapsed:  0:00:29, Remaining:  0:00:31.\n",
      "  Batch   308  of    722.    Elapsed:  0:00:30, Remaining:  0:00:30.\n",
      "  Batch   322  of    722.    Elapsed:  0:00:32, Remaining:  0:00:29.\n",
      "  Batch   336  of    722.    Elapsed:  0:00:33, Remaining:  0:00:28.\n",
      "  Batch   350  of    722.    Elapsed:  0:00:34, Remaining:  0:00:27.\n",
      "  Batch   364  of    722.    Elapsed:  0:00:36, Remaining:  0:00:26.\n",
      "  Batch   378  of    722.    Elapsed:  0:00:37, Remaining:  0:00:25.\n",
      "  Batch   392  of    722.    Elapsed:  0:00:39, Remaining:  0:00:24.\n",
      "  Batch   406  of    722.    Elapsed:  0:00:40, Remaining:  0:00:23.\n",
      "  Batch   420  of    722.    Elapsed:  0:00:41, Remaining:  0:00:22.\n",
      "  Batch   434  of    722.    Elapsed:  0:00:43, Remaining:  0:00:21.\n",
      "  Batch   448  of    722.    Elapsed:  0:00:44, Remaining:  0:00:20.\n",
      "  Batch   462  of    722.    Elapsed:  0:00:45, Remaining:  0:00:19.\n",
      "  Batch   476  of    722.    Elapsed:  0:00:47, Remaining:  0:00:18.\n",
      "  Batch   490  of    722.    Elapsed:  0:00:48, Remaining:  0:00:17.\n",
      "  Batch   504  of    722.    Elapsed:  0:00:49, Remaining:  0:00:16.\n",
      "  Batch   518  of    722.    Elapsed:  0:00:51, Remaining:  0:00:15.\n",
      "  Batch   532  of    722.    Elapsed:  0:00:52, Remaining:  0:00:14.\n",
      "  Batch   546  of    722.    Elapsed:  0:00:54, Remaining:  0:00:13.\n",
      "  Batch   560  of    722.    Elapsed:  0:00:55, Remaining:  0:00:12.\n",
      "  Batch   574  of    722.    Elapsed:  0:00:56, Remaining:  0:00:11.\n",
      "  Batch   588  of    722.    Elapsed:  0:00:58, Remaining:  0:00:10.\n",
      "  Batch   602  of    722.    Elapsed:  0:00:59, Remaining:  0:00:09.\n",
      "  Batch   616  of    722.    Elapsed:  0:01:00, Remaining:  0:00:08.\n",
      "  Batch   630  of    722.    Elapsed:  0:01:02, Remaining:  0:00:07.\n",
      "  Batch   644  of    722.    Elapsed:  0:01:03, Remaining:  0:00:06.\n",
      "  Batch   658  of    722.    Elapsed:  0:01:05, Remaining:  0:00:05.\n",
      "  Batch   672  of    722.    Elapsed:  0:01:06, Remaining:  0:00:04.\n",
      "  Batch   686  of    722.    Elapsed:  0:01:07, Remaining:  0:00:03.\n",
      "  Batch   700  of    722.    Elapsed:  0:01:09, Remaining:  0:00:02.\n",
      "  Batch   714  of    722.    Elapsed:  0:01:10, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.41283456163888493\n",
      "    accuracy: 0.009695290858725761\n",
      "    per class f1: 0.3586354242863945\n",
      "    f1_micro: 0.3586354242863945\n",
      "    f1_macro: 0.8587257617728532\n",
      "    recall_micro: 0.3586354242863945\n",
      "    recall_macro: 0.9293628808864183\n",
      "    precision_micro: 0.3586354242863945\n",
      "    precision_macro: 0.9293628808864183\n",
      "  Training epoch took: 0:01:11\n",
      "\n",
      "  Average evaluation scores:\n",
      "    loss: 0.27922380928482327\n",
      "    accuracy: 0.0\n",
      "    per class f1: 0.3918197278911567\n",
      "    f1_micro: 0.3918197278911567\n",
      "    f1_macro: 0.8638095238095224\n",
      "    recall_micro: 0.3918197278911567\n",
      "    recall_macro: 0.9319047619047631\n",
      "    precision_micro: 0.3918197278911567\n",
      "    precision_macro: 0.9319047619047631\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "  Batch    14  of    722.    Elapsed:  0:00:01, Remaining:  0:00:51.\n",
      "  Batch    28  of    722.    Elapsed:  0:00:03, Remaining:  0:00:50.\n",
      "  Batch    42  of    722.    Elapsed:  0:00:04, Remaining:  0:00:49.\n",
      "  Batch    56  of    722.    Elapsed:  0:00:05, Remaining:  0:00:48.\n",
      "  Batch    70  of    722.    Elapsed:  0:00:07, Remaining:  0:00:47.\n",
      "  Batch    84  of    722.    Elapsed:  0:00:08, Remaining:  0:00:46.\n",
      "  Batch    98  of    722.    Elapsed:  0:00:10, Remaining:  0:00:45.\n",
      "  Batch   112  of    722.    Elapsed:  0:00:11, Remaining:  0:00:44.\n",
      "  Batch   126  of    722.    Elapsed:  0:00:12, Remaining:  0:00:43.\n",
      "  Batch   140  of    722.    Elapsed:  0:00:14, Remaining:  0:00:42.\n",
      "  Batch   154  of    722.    Elapsed:  0:00:15, Remaining:  0:00:41.\n",
      "  Batch   168  of    722.    Elapsed:  0:00:16, Remaining:  0:00:40.\n",
      "  Batch   182  of    722.    Elapsed:  0:00:18, Remaining:  0:00:39.\n",
      "  Batch   196  of    722.    Elapsed:  0:00:19, Remaining:  0:00:38.\n",
      "  Batch   210  of    722.    Elapsed:  0:00:20, Remaining:  0:00:37.\n",
      "  Batch   224  of    722.    Elapsed:  0:00:22, Remaining:  0:00:36.\n",
      "  Batch   238  of    722.    Elapsed:  0:00:23, Remaining:  0:00:35.\n",
      "  Batch   252  of    722.    Elapsed:  0:00:25, Remaining:  0:00:34.\n",
      "  Batch   266  of    722.    Elapsed:  0:00:26, Remaining:  0:00:33.\n",
      "  Batch   280  of    722.    Elapsed:  0:00:27, Remaining:  0:00:32.\n",
      "  Batch   294  of    722.    Elapsed:  0:00:29, Remaining:  0:00:31.\n",
      "  Batch   308  of    722.    Elapsed:  0:00:30, Remaining:  0:00:30.\n",
      "  Batch   322  of    722.    Elapsed:  0:00:31, Remaining:  0:00:29.\n",
      "  Batch   336  of    722.    Elapsed:  0:00:33, Remaining:  0:00:28.\n",
      "  Batch   350  of    722.    Elapsed:  0:00:34, Remaining:  0:00:27.\n",
      "  Batch   364  of    722.    Elapsed:  0:00:35, Remaining:  0:00:26.\n",
      "  Batch   378  of    722.    Elapsed:  0:00:37, Remaining:  0:00:25.\n",
      "  Batch   392  of    722.    Elapsed:  0:00:38, Remaining:  0:00:24.\n",
      "  Batch   406  of    722.    Elapsed:  0:00:40, Remaining:  0:00:23.\n",
      "  Batch   420  of    722.    Elapsed:  0:00:41, Remaining:  0:00:22.\n",
      "  Batch   434  of    722.    Elapsed:  0:00:42, Remaining:  0:00:21.\n",
      "  Batch   448  of    722.    Elapsed:  0:00:44, Remaining:  0:00:20.\n",
      "  Batch   462  of    722.    Elapsed:  0:00:45, Remaining:  0:00:19.\n",
      "  Batch   476  of    722.    Elapsed:  0:00:46, Remaining:  0:00:18.\n",
      "  Batch   490  of    722.    Elapsed:  0:00:48, Remaining:  0:00:17.\n",
      "  Batch   504  of    722.    Elapsed:  0:00:49, Remaining:  0:00:16.\n",
      "  Batch   518  of    722.    Elapsed:  0:00:50, Remaining:  0:00:15.\n",
      "  Batch   532  of    722.    Elapsed:  0:00:52, Remaining:  0:00:14.\n",
      "  Batch   546  of    722.    Elapsed:  0:00:53, Remaining:  0:00:13.\n",
      "  Batch   560  of    722.    Elapsed:  0:00:55, Remaining:  0:00:12.\n",
      "  Batch   574  of    722.    Elapsed:  0:00:56, Remaining:  0:00:11.\n",
      "  Batch   588  of    722.    Elapsed:  0:00:57, Remaining:  0:00:10.\n",
      "  Batch   602  of    722.    Elapsed:  0:00:59, Remaining:  0:00:09.\n",
      "  Batch   616  of    722.    Elapsed:  0:01:00, Remaining:  0:00:08.\n",
      "  Batch   630  of    722.    Elapsed:  0:01:01, Remaining:  0:00:07.\n",
      "  Batch   644  of    722.    Elapsed:  0:01:03, Remaining:  0:00:06.\n",
      "  Batch   658  of    722.    Elapsed:  0:01:04, Remaining:  0:00:05.\n",
      "  Batch   672  of    722.    Elapsed:  0:01:06, Remaining:  0:00:04.\n",
      "  Batch   686  of    722.    Elapsed:  0:01:07, Remaining:  0:00:03.\n",
      "  Batch   700  of    722.    Elapsed:  0:01:08, Remaining:  0:00:02.\n",
      "  Batch   714  of    722.    Elapsed:  0:01:10, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.26569814048191515\n",
      "    accuracy: 0.04016620498614958\n",
      "    per class f1: 0.4239149508540101\n",
      "    f1_micro: 0.4239149508540101\n",
      "    f1_macro: 0.8743305632502302\n",
      "    recall_micro: 0.4239149508540101\n",
      "    recall_macro: 0.9371652816251086\n",
      "    precision_micro: 0.4239149508540101\n",
      "    precision_macro: 0.9371652816251086\n",
      "  Training epoch took: 0:01:10\n",
      "\n",
      "  Average evaluation scores:\n",
      "    loss: 0.23736436649092607\n",
      "    accuracy: 0.12857142857142856\n",
      "    per class f1: 0.5377891156462582\n",
      "    f1_micro: 0.5377891156462582\n",
      "    f1_macro: 0.895714285714285\n",
      "    recall_micro: 0.5377891156462582\n",
      "    recall_macro: 0.947857142857144\n",
      "    precision_micro: 0.5377891156462582\n",
      "    precision_macro: 0.947857142857144\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "  Batch    14  of    722.    Elapsed:  0:00:01, Remaining:  0:00:51.\n",
      "  Batch    28  of    722.    Elapsed:  0:00:03, Remaining:  0:00:50.\n",
      "  Batch    42  of    722.    Elapsed:  0:00:04, Remaining:  0:00:49.\n",
      "  Batch    56  of    722.    Elapsed:  0:00:05, Remaining:  0:00:48.\n",
      "  Batch    70  of    722.    Elapsed:  0:00:07, Remaining:  0:00:47.\n",
      "  Batch    84  of    722.    Elapsed:  0:00:08, Remaining:  0:00:46.\n",
      "  Batch    98  of    722.    Elapsed:  0:00:09, Remaining:  0:00:45.\n",
      "  Batch   112  of    722.    Elapsed:  0:00:11, Remaining:  0:00:44.\n",
      "  Batch   126  of    722.    Elapsed:  0:00:12, Remaining:  0:00:43.\n",
      "  Batch   140  of    722.    Elapsed:  0:00:14, Remaining:  0:00:42.\n",
      "  Batch   154  of    722.    Elapsed:  0:00:15, Remaining:  0:00:41.\n",
      "  Batch   168  of    722.    Elapsed:  0:00:16, Remaining:  0:00:40.\n",
      "  Batch   182  of    722.    Elapsed:  0:00:18, Remaining:  0:00:39.\n",
      "  Batch   196  of    722.    Elapsed:  0:00:19, Remaining:  0:00:38.\n",
      "  Batch   210  of    722.    Elapsed:  0:00:20, Remaining:  0:00:37.\n",
      "  Batch   224  of    722.    Elapsed:  0:00:22, Remaining:  0:00:36.\n",
      "  Batch   238  of    722.    Elapsed:  0:00:23, Remaining:  0:00:35.\n",
      "  Batch   252  of    722.    Elapsed:  0:00:24, Remaining:  0:00:34.\n",
      "  Batch   266  of    722.    Elapsed:  0:00:26, Remaining:  0:00:33.\n",
      "  Batch   280  of    722.    Elapsed:  0:00:27, Remaining:  0:00:32.\n",
      "  Batch   294  of    722.    Elapsed:  0:00:29, Remaining:  0:00:31.\n",
      "  Batch   308  of    722.    Elapsed:  0:00:30, Remaining:  0:00:30.\n",
      "  Batch   322  of    722.    Elapsed:  0:00:31, Remaining:  0:00:29.\n",
      "  Batch   336  of    722.    Elapsed:  0:00:33, Remaining:  0:00:28.\n",
      "  Batch   350  of    722.    Elapsed:  0:00:34, Remaining:  0:00:27.\n",
      "  Batch   364  of    722.    Elapsed:  0:00:35, Remaining:  0:00:26.\n",
      "  Batch   378  of    722.    Elapsed:  0:00:37, Remaining:  0:00:25.\n",
      "  Batch   392  of    722.    Elapsed:  0:00:38, Remaining:  0:00:24.\n",
      "  Batch   406  of    722.    Elapsed:  0:00:39, Remaining:  0:00:23.\n",
      "  Batch   420  of    722.    Elapsed:  0:00:41, Remaining:  0:00:22.\n",
      "  Batch   434  of    722.    Elapsed:  0:00:42, Remaining:  0:00:21.\n",
      "  Batch   448  of    722.    Elapsed:  0:00:44, Remaining:  0:00:20.\n",
      "  Batch   462  of    722.    Elapsed:  0:00:45, Remaining:  0:00:19.\n",
      "  Batch   476  of    722.    Elapsed:  0:00:46, Remaining:  0:00:18.\n",
      "  Batch   490  of    722.    Elapsed:  0:00:48, Remaining:  0:00:17.\n",
      "  Batch   504  of    722.    Elapsed:  0:00:49, Remaining:  0:00:16.\n",
      "  Batch   518  of    722.    Elapsed:  0:00:50, Remaining:  0:00:15.\n",
      "  Batch   532  of    722.    Elapsed:  0:00:52, Remaining:  0:00:14.\n",
      "  Batch   546  of    722.    Elapsed:  0:00:53, Remaining:  0:00:13.\n",
      "  Batch   560  of    722.    Elapsed:  0:00:54, Remaining:  0:00:12.\n",
      "  Batch   574  of    722.    Elapsed:  0:00:56, Remaining:  0:00:11.\n",
      "  Batch   588  of    722.    Elapsed:  0:00:57, Remaining:  0:00:10.\n",
      "  Batch   602  of    722.    Elapsed:  0:00:59, Remaining:  0:00:09.\n",
      "  Batch   616  of    722.    Elapsed:  0:01:00, Remaining:  0:00:08.\n",
      "  Batch   630  of    722.    Elapsed:  0:01:01, Remaining:  0:00:07.\n",
      "  Batch   644  of    722.    Elapsed:  0:01:03, Remaining:  0:00:06.\n",
      "  Batch   658  of    722.    Elapsed:  0:01:04, Remaining:  0:00:05.\n",
      "  Batch   672  of    722.    Elapsed:  0:01:05, Remaining:  0:00:04.\n",
      "  Batch   686  of    722.    Elapsed:  0:01:07, Remaining:  0:00:03.\n",
      "  Batch   700  of    722.    Elapsed:  0:01:08, Remaining:  0:00:02.\n",
      "  Batch   714  of    722.    Elapsed:  0:01:09, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.2121800945767323\n",
      "    accuracy: 0.19806094182825484\n",
      "    per class f1: 0.6032816790780793\n",
      "    f1_micro: 0.6032816790780793\n",
      "    f1_macro: 0.911726685133885\n",
      "    recall_micro: 0.6032816790780793\n",
      "    recall_macro: 0.9558633425669382\n",
      "    precision_micro: 0.6032816790780793\n",
      "    precision_macro: 0.9558633425669382\n",
      "  Training epoch took: 0:01:10\n",
      "\n",
      "  Average evaluation scores:\n",
      "    loss: 0.19863680896482297\n",
      "    accuracy: 0.22142857142857142\n",
      "    per class f1: 0.6076190476190476\n",
      "    f1_micro: 0.6076190476190476\n",
      "    f1_macro: 0.9133333333333326\n",
      "    recall_micro: 0.6076190476190476\n",
      "    recall_macro: 0.9566666666666677\n",
      "    precision_micro: 0.6076190476190476\n",
      "    precision_macro: 0.9566666666666677\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "  Batch    14  of    722.    Elapsed:  0:00:01, Remaining:  0:00:51.\n",
      "  Batch    28  of    722.    Elapsed:  0:00:03, Remaining:  0:00:50.\n",
      "  Batch    42  of    722.    Elapsed:  0:00:04, Remaining:  0:00:49.\n",
      "  Batch    56  of    722.    Elapsed:  0:00:06, Remaining:  0:00:48.\n",
      "  Batch    70  of    722.    Elapsed:  0:00:07, Remaining:  0:00:47.\n",
      "  Batch    84  of    722.    Elapsed:  0:00:08, Remaining:  0:00:46.\n",
      "  Batch    98  of    722.    Elapsed:  0:00:10, Remaining:  0:00:45.\n",
      "  Batch   112  of    722.    Elapsed:  0:00:11, Remaining:  0:00:44.\n",
      "  Batch   126  of    722.    Elapsed:  0:00:13, Remaining:  0:00:43.\n",
      "  Batch   140  of    722.    Elapsed:  0:00:14, Remaining:  0:00:42.\n",
      "  Batch   154  of    722.    Elapsed:  0:00:15, Remaining:  0:00:41.\n",
      "  Batch   168  of    722.    Elapsed:  0:00:17, Remaining:  0:00:40.\n",
      "  Batch   182  of    722.    Elapsed:  0:00:18, Remaining:  0:00:39.\n",
      "  Batch   196  of    722.    Elapsed:  0:00:19, Remaining:  0:00:38.\n",
      "  Batch   210  of    722.    Elapsed:  0:00:21, Remaining:  0:00:37.\n",
      "  Batch   224  of    722.    Elapsed:  0:00:22, Remaining:  0:00:36.\n",
      "  Batch   238  of    722.    Elapsed:  0:00:24, Remaining:  0:00:35.\n",
      "  Batch   252  of    722.    Elapsed:  0:00:25, Remaining:  0:00:34.\n",
      "  Batch   266  of    722.    Elapsed:  0:00:26, Remaining:  0:00:33.\n",
      "  Batch   280  of    722.    Elapsed:  0:00:28, Remaining:  0:00:32.\n",
      "  Batch   294  of    722.    Elapsed:  0:00:29, Remaining:  0:00:31.\n",
      "  Batch   308  of    722.    Elapsed:  0:00:30, Remaining:  0:00:30.\n",
      "  Batch   322  of    722.    Elapsed:  0:00:32, Remaining:  0:00:29.\n",
      "  Batch   336  of    722.    Elapsed:  0:00:33, Remaining:  0:00:28.\n",
      "  Batch   350  of    722.    Elapsed:  0:00:34, Remaining:  0:00:27.\n",
      "  Batch   364  of    722.    Elapsed:  0:00:36, Remaining:  0:00:26.\n",
      "  Batch   378  of    722.    Elapsed:  0:00:37, Remaining:  0:00:25.\n",
      "  Batch   392  of    722.    Elapsed:  0:00:39, Remaining:  0:00:24.\n",
      "  Batch   406  of    722.    Elapsed:  0:00:40, Remaining:  0:00:23.\n",
      "  Batch   420  of    722.    Elapsed:  0:00:41, Remaining:  0:00:22.\n",
      "  Batch   434  of    722.    Elapsed:  0:00:43, Remaining:  0:00:21.\n",
      "  Batch   448  of    722.    Elapsed:  0:00:44, Remaining:  0:00:20.\n",
      "  Batch   462  of    722.    Elapsed:  0:00:45, Remaining:  0:00:19.\n",
      "  Batch   476  of    722.    Elapsed:  0:00:47, Remaining:  0:00:18.\n",
      "  Batch   490  of    722.    Elapsed:  0:00:48, Remaining:  0:00:17.\n",
      "  Batch   504  of    722.    Elapsed:  0:00:49, Remaining:  0:00:16.\n",
      "  Batch   518  of    722.    Elapsed:  0:00:51, Remaining:  0:00:15.\n",
      "  Batch   532  of    722.    Elapsed:  0:00:52, Remaining:  0:00:14.\n",
      "  Batch   546  of    722.    Elapsed:  0:00:54, Remaining:  0:00:13.\n",
      "  Batch   560  of    722.    Elapsed:  0:00:55, Remaining:  0:00:12.\n",
      "  Batch   574  of    722.    Elapsed:  0:00:56, Remaining:  0:00:11.\n",
      "  Batch   588  of    722.    Elapsed:  0:00:58, Remaining:  0:00:10.\n",
      "  Batch   602  of    722.    Elapsed:  0:00:59, Remaining:  0:00:09.\n",
      "  Batch   616  of    722.    Elapsed:  0:01:00, Remaining:  0:00:08.\n",
      "  Batch   630  of    722.    Elapsed:  0:01:02, Remaining:  0:00:07.\n",
      "  Batch   644  of    722.    Elapsed:  0:01:03, Remaining:  0:00:06.\n",
      "  Batch   658  of    722.    Elapsed:  0:01:04, Remaining:  0:00:05.\n",
      "  Batch   672  of    722.    Elapsed:  0:01:06, Remaining:  0:00:04.\n",
      "  Batch   686  of    722.    Elapsed:  0:01:07, Remaining:  0:00:03.\n",
      "  Batch   700  of    722.    Elapsed:  0:01:09, Remaining:  0:00:02.\n",
      "  Batch   714  of    722.    Elapsed:  0:01:10, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.1744315595652423\n",
      "    accuracy: 0.28254847645429365\n",
      "    per class f1: 0.6970275569306056\n",
      "    f1_micro: 0.6970275569306056\n",
      "    f1_macro: 0.9316712834718338\n",
      "    recall_micro: 0.6970275569306056\n",
      "    recall_macro: 0.9658356417359153\n",
      "    precision_micro: 0.6970275569306056\n",
      "    precision_macro: 0.9658356417359153\n",
      "  Training epoch took: 0:01:11\n",
      "\n",
      "  Average evaluation scores:\n",
      "    loss: 0.183704665516104\n",
      "    accuracy: 0.2857142857142857\n",
      "    per class f1: 0.6584013605442176\n",
      "    f1_micro: 0.6584013605442176\n",
      "    f1_macro: 0.9238095238095233\n",
      "    recall_micro: 0.6584013605442176\n",
      "    recall_macro: 0.961904761904763\n",
      "    precision_micro: 0.6584013605442176\n",
      "    precision_macro: 0.961904761904763\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "  Batch    14  of    722.    Elapsed:  0:00:02, Remaining:  0:01:41.\n",
      "  Batch    28  of    722.    Elapsed:  0:00:03, Remaining:  0:01:39.\n",
      "  Batch    42  of    722.    Elapsed:  0:00:05, Remaining:  0:01:37.\n",
      "  Batch    56  of    722.    Elapsed:  0:00:06, Remaining:  0:01:35.\n",
      "  Batch    70  of    722.    Elapsed:  0:00:08, Remaining:  0:00:47.\n",
      "  Batch    84  of    722.    Elapsed:  0:00:09, Remaining:  0:00:46.\n",
      "  Batch    98  of    722.    Elapsed:  0:00:11, Remaining:  0:00:45.\n",
      "  Batch   112  of    722.    Elapsed:  0:00:12, Remaining:  0:01:27.\n",
      "  Batch   126  of    722.    Elapsed:  0:00:14, Remaining:  0:01:25.\n",
      "  Batch   140  of    722.    Elapsed:  0:00:15, Remaining:  0:01:23.\n",
      "  Batch   154  of    722.    Elapsed:  0:00:17, Remaining:  0:01:21.\n",
      "  Batch   168  of    722.    Elapsed:  0:00:19, Remaining:  0:01:19.\n",
      "  Batch   182  of    722.    Elapsed:  0:00:20, Remaining:  0:01:17.\n",
      "  Batch   196  of    722.    Elapsed:  0:00:22, Remaining:  0:00:38.\n",
      "  Batch   210  of    722.    Elapsed:  0:00:23, Remaining:  0:00:37.\n",
      "  Batch   224  of    722.    Elapsed:  0:00:24, Remaining:  0:00:36.\n",
      "  Batch   238  of    722.    Elapsed:  0:00:26, Remaining:  0:00:35.\n",
      "  Batch   252  of    722.    Elapsed:  0:00:27, Remaining:  0:00:34.\n",
      "  Batch   266  of    722.    Elapsed:  0:00:29, Remaining:  0:00:33.\n",
      "  Batch   280  of    722.    Elapsed:  0:00:30, Remaining:  0:00:32.\n",
      "  Batch   294  of    722.    Elapsed:  0:00:32, Remaining:  0:01:01.\n",
      "  Batch   308  of    722.    Elapsed:  0:00:33, Remaining:  0:00:30.\n",
      "  Batch   322  of    722.    Elapsed:  0:00:35, Remaining:  0:00:29.\n",
      "  Batch   336  of    722.    Elapsed:  0:00:36, Remaining:  0:00:28.\n",
      "  Batch   350  of    722.    Elapsed:  0:00:38, Remaining:  0:00:27.\n",
      "  Batch   364  of    722.    Elapsed:  0:00:39, Remaining:  0:00:26.\n",
      "  Batch   378  of    722.    Elapsed:  0:00:41, Remaining:  0:00:25.\n",
      "  Batch   392  of    722.    Elapsed:  0:00:42, Remaining:  0:00:24.\n",
      "  Batch   406  of    722.    Elapsed:  0:00:43, Remaining:  0:00:23.\n",
      "  Batch   420  of    722.    Elapsed:  0:00:45, Remaining:  0:00:22.\n",
      "  Batch   434  of    722.    Elapsed:  0:00:46, Remaining:  0:00:21.\n",
      "  Batch   448  of    722.    Elapsed:  0:00:48, Remaining:  0:00:20.\n",
      "  Batch   462  of    722.    Elapsed:  0:00:49, Remaining:  0:00:19.\n",
      "  Batch   476  of    722.    Elapsed:  0:00:51, Remaining:  0:00:18.\n",
      "  Batch   490  of    722.    Elapsed:  0:00:52, Remaining:  0:00:17.\n",
      "  Batch   504  of    722.    Elapsed:  0:00:53, Remaining:  0:00:16.\n",
      "  Batch   518  of    722.    Elapsed:  0:00:55, Remaining:  0:00:15.\n",
      "  Batch   532  of    722.    Elapsed:  0:00:56, Remaining:  0:00:14.\n",
      "  Batch   546  of    722.    Elapsed:  0:00:57, Remaining:  0:00:13.\n",
      "  Batch   560  of    722.    Elapsed:  0:00:59, Remaining:  0:00:12.\n",
      "  Batch   574  of    722.    Elapsed:  0:01:00, Remaining:  0:00:11.\n",
      "  Batch   588  of    722.    Elapsed:  0:01:01, Remaining:  0:00:10.\n",
      "  Batch   602  of    722.    Elapsed:  0:01:03, Remaining:  0:00:09.\n",
      "  Batch   616  of    722.    Elapsed:  0:01:04, Remaining:  0:00:08.\n",
      "  Batch   630  of    722.    Elapsed:  0:01:06, Remaining:  0:00:07.\n",
      "  Batch   644  of    722.    Elapsed:  0:01:07, Remaining:  0:00:06.\n",
      "  Batch   658  of    722.    Elapsed:  0:01:08, Remaining:  0:00:05.\n",
      "  Batch   672  of    722.    Elapsed:  0:01:10, Remaining:  0:00:04.\n",
      "  Batch   686  of    722.    Elapsed:  0:01:11, Remaining:  0:00:03.\n",
      "  Batch   700  of    722.    Elapsed:  0:01:12, Remaining:  0:00:02.\n",
      "  Batch   714  of    722.    Elapsed:  0:01:14, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.14851417397651975\n",
      "    accuracy: 0.3559556786703601\n",
      "    per class f1: 0.7528849036466781\n",
      "    f1_micro: 0.7528849036466781\n",
      "    f1_macro: 0.9443213296398855\n",
      "    recall_micro: 0.7528849036466781\n",
      "    recall_macro: 0.9721606648199422\n",
      "    precision_micro: 0.7528849036466781\n",
      "    precision_macro: 0.9721606648199422\n",
      "  Training epoch took: 0:01:14\n",
      "\n",
      "  Average evaluation scores:\n",
      "    loss: 0.17467488780883805\n",
      "    accuracy: 0.29285714285714287\n",
      "    per class f1: 0.6820918367346939\n",
      "    f1_micro: 0.6820918367346939\n",
      "    f1_macro: 0.9285714285714288\n",
      "    recall_micro: 0.6820918367346939\n",
      "    recall_macro: 0.9642857142857151\n",
      "    precision_micro: 0.6820918367346939\n",
      "    precision_macro: 0.9642857142857151\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "  Batch    14  of    722.    Elapsed:  0:00:01, Remaining:  0:00:51.\n",
      "  Batch    28  of    722.    Elapsed:  0:00:03, Remaining:  0:00:50.\n",
      "  Batch    42  of    722.    Elapsed:  0:00:04, Remaining:  0:00:49.\n",
      "  Batch    56  of    722.    Elapsed:  0:00:06, Remaining:  0:00:48.\n",
      "  Batch    70  of    722.    Elapsed:  0:00:07, Remaining:  0:00:47.\n",
      "  Batch    84  of    722.    Elapsed:  0:00:08, Remaining:  0:00:46.\n",
      "  Batch    98  of    722.    Elapsed:  0:00:10, Remaining:  0:00:45.\n",
      "  Batch   112  of    722.    Elapsed:  0:00:11, Remaining:  0:00:44.\n",
      "  Batch   126  of    722.    Elapsed:  0:00:13, Remaining:  0:00:43.\n",
      "  Batch   140  of    722.    Elapsed:  0:00:14, Remaining:  0:00:42.\n",
      "  Batch   154  of    722.    Elapsed:  0:00:16, Remaining:  0:00:41.\n",
      "  Batch   168  of    722.    Elapsed:  0:00:17, Remaining:  0:00:40.\n",
      "  Batch   182  of    722.    Elapsed:  0:00:18, Remaining:  0:00:39.\n",
      "  Batch   196  of    722.    Elapsed:  0:00:20, Remaining:  0:00:38.\n",
      "  Batch   210  of    722.    Elapsed:  0:00:21, Remaining:  0:00:37.\n",
      "  Batch   224  of    722.    Elapsed:  0:00:23, Remaining:  0:00:36.\n",
      "  Batch   238  of    722.    Elapsed:  0:00:24, Remaining:  0:00:35.\n",
      "  Batch   252  of    722.    Elapsed:  0:00:26, Remaining:  0:00:34.\n",
      "  Batch   266  of    722.    Elapsed:  0:00:27, Remaining:  0:00:33.\n",
      "  Batch   280  of    722.    Elapsed:  0:00:28, Remaining:  0:00:32.\n",
      "  Batch   294  of    722.    Elapsed:  0:00:30, Remaining:  0:00:31.\n",
      "  Batch   308  of    722.    Elapsed:  0:00:31, Remaining:  0:00:30.\n",
      "  Batch   322  of    722.    Elapsed:  0:00:33, Remaining:  0:00:29.\n",
      "  Batch   336  of    722.    Elapsed:  0:00:34, Remaining:  0:00:28.\n",
      "  Batch   350  of    722.    Elapsed:  0:00:35, Remaining:  0:00:27.\n",
      "  Batch   364  of    722.    Elapsed:  0:00:37, Remaining:  0:00:26.\n",
      "  Batch   378  of    722.    Elapsed:  0:00:38, Remaining:  0:00:25.\n",
      "  Batch   392  of    722.    Elapsed:  0:00:40, Remaining:  0:00:24.\n",
      "  Batch   406  of    722.    Elapsed:  0:00:41, Remaining:  0:00:23.\n",
      "  Batch   420  of    722.    Elapsed:  0:00:42, Remaining:  0:00:22.\n",
      "  Batch   434  of    722.    Elapsed:  0:00:44, Remaining:  0:00:21.\n",
      "  Batch   448  of    722.    Elapsed:  0:00:45, Remaining:  0:00:20.\n",
      "  Batch   462  of    722.    Elapsed:  0:00:46, Remaining:  0:00:19.\n",
      "  Batch   476  of    722.    Elapsed:  0:00:48, Remaining:  0:00:18.\n",
      "  Batch   490  of    722.    Elapsed:  0:00:49, Remaining:  0:00:17.\n",
      "  Batch   504  of    722.    Elapsed:  0:00:51, Remaining:  0:00:16.\n",
      "  Batch   518  of    722.    Elapsed:  0:00:52, Remaining:  0:00:15.\n",
      "  Batch   532  of    722.    Elapsed:  0:00:53, Remaining:  0:00:14.\n",
      "  Batch   546  of    722.    Elapsed:  0:00:55, Remaining:  0:00:13.\n",
      "  Batch   560  of    722.    Elapsed:  0:00:56, Remaining:  0:00:12.\n",
      "  Batch   574  of    722.    Elapsed:  0:00:57, Remaining:  0:00:11.\n",
      "  Batch   588  of    722.    Elapsed:  0:00:59, Remaining:  0:00:10.\n",
      "  Batch   602  of    722.    Elapsed:  0:01:00, Remaining:  0:00:09.\n",
      "  Batch   616  of    722.    Elapsed:  0:01:01, Remaining:  0:00:08.\n",
      "  Batch   630  of    722.    Elapsed:  0:01:03, Remaining:  0:00:07.\n",
      "  Batch   644  of    722.    Elapsed:  0:01:04, Remaining:  0:00:06.\n",
      "  Batch   658  of    722.    Elapsed:  0:01:05, Remaining:  0:00:05.\n",
      "  Batch   672  of    722.    Elapsed:  0:01:07, Remaining:  0:00:04.\n",
      "  Batch   686  of    722.    Elapsed:  0:01:08, Remaining:  0:00:03.\n",
      "  Batch   700  of    722.    Elapsed:  0:01:10, Remaining:  0:00:02.\n",
      "  Batch   714  of    722.    Elapsed:  0:01:11, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.12884728370332404\n",
      "    accuracy: 0.4473684210526316\n",
      "    per class f1: 0.8012245026441713\n",
      "    f1_micro: 0.8012245026441713\n",
      "    f1_macro: 0.9543859649122766\n",
      "    recall_micro: 0.8012245026441713\n",
      "    recall_macro: 0.9771929824561387\n",
      "    precision_micro: 0.8012245026441713\n",
      "    precision_macro: 0.9771929824561387\n",
      "  Training epoch took: 0:01:12\n",
      "\n",
      "  Average evaluation scores:\n",
      "    loss: 0.1735825128587229\n",
      "    accuracy: 0.2857142857142857\n",
      "    per class f1: 0.6827380952380953\n",
      "    f1_micro: 0.6827380952380953\n",
      "    f1_macro: 0.9276190476190475\n",
      "    recall_micro: 0.6827380952380953\n",
      "    recall_macro: 0.9638095238095247\n",
      "    precision_micro: 0.6827380952380953\n",
      "    precision_macro: 0.9638095238095247\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "======== Epoch 7 / 20 ========\n",
      "  Batch    14  of    722.    Elapsed:  0:00:01, Remaining:  0:00:51.\n"
     ]
    }
   ],
   "source": [
    "train_stats, test_stats = train_run(\n",
    "    model, device, train_dataloader, test_dataloader, \n",
    "    id2label, forward_args, optimizer, scheduler, \n",
    "    EPOCHS, EMPTY_CACHE, \n",
    "    calc_metrics=CALC_METRICS, \n",
    "    use_tqdm=False,\n",
    "    checkpoint_path=CHECKPOINT_PATH\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
